{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparing BERT and T5: Understanding Tokenization and Summarization**\n",
    "\n",
    "### **Table of Contents**\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Setup and Installation](#2-setup-and-installation)\n",
    "3. [BERT: Tokenization and Masked Language Modeling](#3-bert-tokenization-and-masked-language-modeling)\n",
    "4. [T5: Summarization](#4-t5-summarization)\n",
    "5. [Comparative Analysis](#5-comparative-analysis)\n",
    "6. [Conclusion](#6-conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction**\n",
    "\n",
    "In the realm of Natural Language Processing (NLP), **BERT** and **T5** are two groundbreaking transformer-based models developed by Google. Despite both leveraging the transformer architecture, they are tailored for distinct tasks and exhibit unique strengths:\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers):**\n",
    "  - **Purpose:** Designed for understanding and interpreting text.\n",
    "  - **Functionality:** Excels in tasks like question answering, sentiment analysis, and Masked Language Modeling (MLM), where it predicts missing tokens in a sentence.\n",
    "  - **Strength:** Deep contextual understanding of language.\n",
    "\n",
    "- **T5 (Text-To-Text Transfer Transformer):**\n",
    "  - **Purpose:** Crafted as a unified framework to handle a variety of NLP tasks by converting them into a text-to-text format.\n",
    "  - **Functionality:** Capable of performing tasks such as translation, summarization, paraphrasing, and more.\n",
    "  - **Strength:** Versatile text generation capabilities, particularly in summarization.\n",
    "\n",
    "This notebook aims to demonstrate the distinct strengths of **BERT** and **T5** by showcasing **BERT's** tokenizer and MLM capabilities, and **T5's** prowess in summarizing text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Setup and Installation**\n",
    "\n",
    "Before delving into the models, ensure that the necessary libraries are installed. We'll be utilizing Hugging Face's `transformers` library, which provides seamless access to pre-trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. BERT: Tokenization and Masked Language Modeling**\n",
    "\n",
    "**BERT** is renowned for its ability to understand the context of words within a sentence. It achieves this through **Masked Language Modeling (MLM)**, where it predicts missing tokens in a given text. Additionally, BERT's tokenizer plays a crucial role in preparing text data for the model.\n",
    "\n",
    "#### **3.1. BERT Tokenizer Demonstration**\n",
    "\n",
    "Let's start by exploring **BERT's** tokenizer. The tokenizer breaks down sentences into tokens (subwords) that BERT can process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Tokenized Sentence:\n",
      "['nearly', 'all', 'men', 'can', 'stand', 'ad', '##vers', '##ity', ',', 'but', 'if', 'you', 'want', 'to', 'test', 'a', 'man', \"'\", 's', 'character', ',', 'give', 'him', 'power', '.']\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "bert_example_sentence = \"Nearly all men can stand adversity, but if you want to test a man's character, give him power.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "bert_tokens = bert_tokenizer.tokenize(bert_example_sentence)\n",
    "print(f\"BERT Tokenized Sentence:\\n{bert_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2. BERT Masked Language Modeling**\n",
    "\n",
    "Now, let's demonstrate **BERT's** ability to predict masked tokens in a sentence. We'll replace certain words with `[MASK]` tokens and let BERT predict the most probable replacements.\n",
    "\n",
    "**Original Sentence:**\n",
    "\"Nearly all men can stand adversity, but if you want to test a man's character, give him power.\"\n",
    "\n",
    "**Masked Sentence:**\n",
    "\"Nearly all men can [MASK], but if you want to test a man's [MASK], give him power.\"\n",
    "\n",
    "##### **3.2.1. BERT Model Implementation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Filled Sentence: Nearly all men can fight, but if you want to test a man's strength, give him power.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Masked sentence\n",
    "bert_masked_sentence = \"Nearly all men can [MASK], but if you want to test a man's [MASK], give him power.\"\n",
    "\n",
    "# Tokenize the input sentence\n",
    "bert_input_ids = bert_tokenizer.encode(bert_masked_sentence, return_tensors='pt')\n",
    "\n",
    "# Identify the [MASK] token indices\n",
    "mask_token_indices = torch.where(bert_input_ids == bert_tokenizer.mask_token_id)[1]\n",
    "\n",
    "# Initialize a list to store predicted words\n",
    "bert_predicted_words = []\n",
    "\n",
    "# Iterate over each [MASK] token to predict the missing word\n",
    "for mask_index in mask_token_indices:\n",
    "    # Clone the input_ids to avoid modifying the original tensor\n",
    "    bert_input_ids_clone = bert_input_ids.clone()\n",
    "    \n",
    "    # Forward pass through the model to get logits\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(bert_input_ids_clone)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Extract logits for the current mask position\n",
    "    mask_logits = logits[0, mask_index, :]\n",
    "    \n",
    "    # Pick the highest scoring token\n",
    "    predicted_token_id = torch.argmax(mask_logits).item()\n",
    "    predicted_word = bert_tokenizer.decode([predicted_token_id]).strip()\n",
    "    bert_predicted_words.append(predicted_word)\n",
    "\n",
    "# Replace [MASK] tokens with the predicted words\n",
    "filled_bert_sentence = bert_masked_sentence\n",
    "for word in bert_predicted_words:\n",
    "    filled_bert_sentence = filled_bert_sentence.replace(\"[MASK]\", word, 1)\n",
    "\n",
    "print(f\"BERT Filled Sentence: {filled_bert_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "```\n",
    "BERT Filled Sentence: Nearly all men can fight, but if you want to test a man's strength, give him power.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3.2.2. Explanation**\n",
    "\n",
    "1. **Tokenization:** The masked sentence is tokenized, and the positions of `[MASK]` tokens are identified.\n",
    "2. **Prediction:** For each `[MASK]`, BERT predicts the most probable word based on the surrounding context.\n",
    "3. **Replacement:** The predicted words replace the `[MASK]` tokens to form the complete, filled sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. T5: Summarization**\n",
    "\n",
    "**T5** adopts a different approach by treating every NLP task as a text generation problem. This design allows it to handle tasks like translation, summarization, and paraphrasing with remarkable flexibility. Here, we'll focus on **T5's** summarization capability.\n",
    "\n",
    "#### **4.1. T5 Summarization Implementation**\n",
    "\n",
    "Let's use **T5** to summarize a paragraph. We'll provide a longer text and ask **T5** to generate a concise summary.\n",
    "\n",
    "##### **4.1.1. Summarization Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Generated Summary:\n",
      "if you want to test a man's integrity, give him power.\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Example paragraph to summarize\n",
    "t5_input_text = \"\"\"\n",
    "Paraphrase: Nearly all men can stand adversity, but if you want to test a man's integrity, give him power.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the input with the 'summarize:' prefix\n",
    "t5_input_text_prefixed = \"summarize: \" + t5_input_text.strip()\n",
    "\n",
    "# Tokenize the input\n",
    "t5_input_ids = t5_tokenizer.encode(t5_input_text_prefixed, return_tensors='pt')\n",
    "\n",
    "# Generate the summary using beam search for better results\n",
    "t5_output_ids = t5_model.generate(\n",
    "    t5_input_ids, \n",
    "    max_length=50,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the generated output, skipping special tokens\n",
    "t5_summary = t5_tokenizer.decode(t5_output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"T5 Generated Summary:\\n{t5_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "```\n",
    "T5 Generated Summary:\n",
    "if you want to test a man's integrity, give him power.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4.1.2. Explanation**\n",
    "\n",
    "1. **Input Preparation:** The paragraph to be summarized is prefixed with `\"summarize: \"` to inform **T5** of the desired task.\n",
    "2. **Tokenization:** The prefixed text is tokenized into input IDs that **T5** can process.\n",
    "3. **Generation:** **T5** generates a summary using beam search (`num_beams=4`) to enhance the quality of the output. The `early_stopping=True` parameter ensures that the generation process stops once a sufficient summary is produced.\n",
    "4. **Decoding:** The generated summary is decoded from token IDs back into readable text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Comparative Analysis**\n",
    "\n",
    "Having implemented both **BERT** and **T5** for their respective strengths, let's compare their functionalities and use cases.\n",
    "\n",
    "#### **5.1. Side-by-Side Comparison**\n",
    "\n",
    "| **Aspect**                       | **BERT**                                                                 | **T5**                                                          |\n",
    "|----------------------------------|--------------------------------------------------------------------------|-----------------------------------------------------------------|\n",
    "| **Primary Function**             | Masked token prediction                                                  | Text-to-text generation (e.g., summarization, translation)      |\n",
    "| **Handling Multiple Masks**      | Requires separate predictions for each `[MASK]`                         | Handles multiple tasks within a single framework                |\n",
    "| **Use Cases**                    | Understanding tasks (e.g., sentiment analysis, question answering)       | Generation tasks (e.g., summarization, translation, paraphrasing) |\n",
    "| **Flexibility**                  | Focused on comprehension and token prediction                           | Highly versatile across various NLP tasks                       |\n",
    "| **Output Nature**                | Predicts discrete tokens                                                | Generates coherent and concise text spans                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2. Performance Insights**\n",
    "\n",
    "- **Efficiency:**\n",
    "  - **BERT:** Excels at tasks requiring deep understanding and token-level predictions but needs iterative processing for multiple masked tokens.\n",
    "  - **T5:** More efficient for generation tasks, especially those involving multiple outputs like summarization, as it can handle them in a single pass.\n",
    "\n",
    "- **Flexibility:**\n",
    "  - **BERT:** Best suited for tasks that involve analyzing and understanding existing text.\n",
    "  - **T5:** Offers a unified approach to various tasks, making it suitable for generating new text based on given inputs.\n",
    "\n",
    "- **Contextual Understanding:**\n",
    "  - **BERT:** Provides rich contextual embeddings, making it ideal for tasks that rely on understanding the intricacies of language.\n",
    "  - **T5:** Utilizes its generative capabilities to produce contextually appropriate and coherent summaries or translations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've delved into the distinct functionalities of **BERT** and **T5**:\n",
    "\n",
    "- **BERT** is a formidable model for understanding and predicting missing tokens within a sentence. Its tokenizer efficiently breaks down text into manageable tokens, and its **Masked Language Modeling** capability allows it to predict and fill in gaps based on contextual clues. This makes **BERT** exceptionally suited for tasks that require deep comprehension and analysis of text.\n",
    "\n",
    "- **T5**, on the other hand, offers a versatile framework for a wide array of NLP tasks by treating them as text-to-text problems. Its strength lies in its ability to generate coherent and concise summaries, making it invaluable for tasks like summarization, translation, and paraphrasing. **T5's** generative nature allows it to produce new text based on given inputs, showcasing its flexibility and power in handling diverse language tasks.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **BERT's** tokenizer and **Masked Language Modeling** are ideal for tasks that require understanding and predicting specific parts of the text.\n",
    "- **T5's** design as a text-to-text model makes it highly effective for generating new content, such as summaries, translations, and paraphrases.\n",
    "- Choosing between **BERT** and **T5** depends on the specific requirements of your NLP projectâ€”whether it leans more towards understanding existing text or generating new text based on inputs.\n",
    "\n",
    "By leveraging the strengths of both models, you can build robust NLP applications that harness the power of deep language understanding and sophisticated text generation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
